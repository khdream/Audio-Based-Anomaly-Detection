{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class DataTransformationConfig:\n",
    "#     root_dir: Path\n",
    "#     data_path: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    normal_data_path: Path\n",
    "    abnormal_data_path: Path\n",
    "    normal_features_path: Path\n",
    "    abnormal_features_path: Path\n",
    "    feature_names: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Anomaly_Detection.constants import *\n",
    "from Anomaly_Detection.utils.common import read_yaml, create_directories,save_bin,load_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            normal_data_path=config.normal_data_path,\n",
    "            abnormal_data_path=config.abnormal_data_path,\n",
    "            normal_features_path= config.normal_features_path,\n",
    "            abnormal_features_path= config.abnormal_features_path,\n",
    "            feature_names= config.feature_names\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "from Anomaly_Detection import logger\n",
    "from Anomaly_Detection.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_audio_files(self,path, label):\n",
    "        audio_files = []\n",
    "        labels = []\n",
    "        logger.info(f\"Loading audio data from {path} into file.\")\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(path, filename)\n",
    "                audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "                audio_files.append(audio)\n",
    "                labels.append(label)\n",
    "        return audio_files, labels, sample_rate\n",
    "\n",
    "    def extract_mfccs(self,audio, sample_rate, n_mfcc=13):\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "        return mfccs_processed\n",
    "    \n",
    "    def extract_spectral_features(self,audio, sample_rate):\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)[0]\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)[0]\n",
    "        return np.mean(spectral_centroids), np.mean(spectral_rolloff), np.mean(spectral_contrast)\n",
    "    \n",
    "    def extract_temporal_features(self,audio):\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "        autocorrelation = librosa.autocorrelate(audio)\n",
    "        return np.mean(zero_crossing_rate), np.mean(autocorrelation)\n",
    "    \n",
    "    def extract_additional_features(self,audio, sample_rate):\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate)\n",
    "        spec_flatness = librosa.feature.spectral_flatness(y=audio)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        \n",
    "        return np.mean(chroma_stft), np.mean(spec_bw), np.mean(spec_flatness), np.mean(rolloff), np.mean(rms)\n",
    "\n",
    "    def extract_all_features(self,audio_data, sample_rate):\n",
    "        features = []\n",
    "        logger.info(f\"Extracting features from audio_data.\")\n",
    "        for audio in audio_data:\n",
    "            mfccs = self.extract_mfccs(audio, sample_rate)\n",
    "            spectral_features = self.extract_spectral_features(audio, sample_rate)\n",
    "            temporal_features = self.extract_temporal_features(audio)\n",
    "            additional_features = self.extract_additional_features(audio, sample_rate)\n",
    "            all_features = np.concatenate([mfccs, spectral_features, temporal_features, additional_features])\n",
    "            features.append(all_features)\n",
    "        return np.array(features)\n",
    "    \n",
    "    def feature_list(self):\n",
    "        # Assuming you have 13 MFCCs\n",
    "        n_mfcc = 13\n",
    "        mfcc_labels = [f'MFCC_{i+1}' for i in range(n_mfcc)]\n",
    "\n",
    "        # We have 3 spectral features\n",
    "        spectral_labels = ['Spectral Centroid', 'Spectral Rolloff', 'Spectral Contrast']\n",
    "\n",
    "        # We have 2 temporal features\n",
    "        temporal_labels = ['Zero Crossing Rate', 'Autocorrelation']\n",
    "\n",
    "        # Adding additional features to features list\n",
    "        additional_features = ['Chroma Features', 'Spectral Bandwidth', 'Spectral Flatness', 'Spectral Roll-off Frequency', 'Root Mean Square Energy']\n",
    "\n",
    "        feature_names = mfcc_labels + spectral_labels + temporal_labels + additional_features\n",
    "        return  feature_names\n",
    "\n",
    "\n",
    "    def data_preprocessing(self):\n",
    "\n",
    "        logger.info(f\"Starting Data preprocessing\")\n",
    "        abnormal_pump_path = self.config.abnormal_data_path\n",
    "        normal_pump_path= self.config.normal_data_path\n",
    "\n",
    "        print(abnormal_pump_path)\n",
    "        print(normal_pump_path)\n",
    "  \n",
    "        # Load the datasets\n",
    "        abnormal_audio, abnormal_labels, _ = self.load_audio_files(abnormal_pump_path, label=1)\n",
    "        normal_audio, normal_labels, sample_rate = self.load_audio_files(normal_pump_path, label=0)\n",
    "\n",
    "        logger.info(f\"Size of abnormal_audio: {len(abnormal_audio)}.\")\n",
    "        logger.info(f\"Size of normal_audio: {len(normal_audio)}.\")\n",
    "\n",
    "        # Extract features for both normal and abnormal data\n",
    "        normal_features = self.extract_all_features(normal_audio, sample_rate)\n",
    "        abnormal_features = self.extract_all_features(abnormal_audio, sample_rate)\n",
    "        feature_names = self.feature_list()\n",
    "\n",
    "        joblib.dump(normal_features,(os.path.join(self.config.root_dir, \"normal_features.pkl\")))\n",
    "        joblib.dump(abnormal_features,(os.path.join(self.config.root_dir, \"abnormal_features.pkl\")))\n",
    "        joblib.dump(feature_names,(os.path.join(self.config.root_dir, \"feature_names.pkl\")))\n",
    "        logger.info(f\"Data preprocessing completed.\")\n",
    "\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "\n",
    "        # Load normal_features.pkl\n",
    "        normal_features = joblib.load(os.path.join(self.config.root_dir, \"normal_features.pkl\"))\n",
    "        logger.info(f\"Loaded normal features {normal_features.shape}.\")\n",
    "        # Load abnormal_features.pkl\n",
    "        abnormal_features = joblib.load(os.path.join(self.config.root_dir, \"abnormal_features.pkl\"))\n",
    "        logger.info(f\"Loaded abnormal features {abnormal_features.shape}.\")\n",
    "\n",
    "        X_train, X_val = train_test_split(normal_features, test_size=0.2, random_state=42)\n",
    "        X_test = abnormal_features\n",
    "        scaler = StandardScaler()\n",
    "        # Fit the scaler on the training data and transform both training, validation, and test sets\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        # Combine normal and abnormal data\n",
    "        X_combined_test = np.concatenate((X_val_scaled, X_test_scaled))\n",
    "        y_combined_test = np.concatenate((np.zeros(len(X_val_scaled)), np.ones(len(X_test_scaled))))  # 0 for normal, 1 for abnormal\n",
    "\n",
    "        joblib.dump(X_train_scaled,(os.path.join(self.config.root_dir, \"X_train_scaled.pkl\")))\n",
    "        joblib.dump(X_val_scaled,(os.path.join(self.config.root_dir, \"X_val_scaled.pkl\")))\n",
    "        joblib.dump(X_combined_test,(os.path.join(self.config.root_dir, \"X_combined_test.pkl\")))\n",
    "        joblib.dump(y_combined_test,(os.path.join(self.config.root_dir, \"y_combined_test.pkl\")))\n",
    "        \n",
    "        logger.info(\"Splited data into training and test sets\")\n",
    "        logger.info(f\"Saved X_train_scaled {X_train_scaled.shape} into file.\")\n",
    "        logger.info(f\"Saved X_train_scaled {X_val_scaled.shape} into file.\")\n",
    "        logger.info(f\"Saved X_combined_test {X_combined_test.shape} into file.\")\n",
    "        logger.info(f\"Saved y_combined_test {y_combined_test.shape} into file.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-02 17:47:23,729: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-12-02 17:47:23,738: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-02 17:47:23,744: INFO: common: created directory at: artifacts]\n",
      "[2023-12-02 17:47:23,751: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-12-02 17:47:23,757: INFO: 3609834075: Loaded normal features (381, 23).]\n",
      "[2023-12-02 17:47:23,762: INFO: 3609834075: Loaded abnormal features (138, 23).]\n",
      "[2023-12-02 17:47:23,787: INFO: 3609834075: Splited data into training and test sets]\n",
      "[2023-12-02 17:47:23,791: INFO: 3609834075: Saved X_train_scaled (304, 23) into file.]\n",
      "[2023-12-02 17:47:23,795: INFO: 3609834075: Saved X_train_scaled (77, 23) into file.]\n",
      "[2023-12-02 17:47:23,800: INFO: 3609834075: Saved X_combined_test (215, 23) into file.]\n",
      "[2023-12-02 17:47:23,804: INFO: 3609834075: Saved y_combined_test (215,) into file.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    #data_transformation.data_preprocessing()\n",
    "    data_transformation.train_test_spliting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
