{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/JAISON14/Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC.mlflow\" \n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"JAISON14\" \n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"bd73d8bfc9f7a55ee6faf6cdad0bb66177f5f752\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(\"artifacts/training/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    trained_model_path: Path\n",
    "    root_dir: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    feature_names_path: Path\n",
    "    feature_importance_path: Path\n",
    "    X_combined_test_path: Path\n",
    "    y_combined_test_path: Path\n",
    "    scores_path : Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_feature_count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Anomaly_Detection.constants import *\n",
    "from Anomaly_Detection.utils.common import read_yaml, create_directories,save_bin,load_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        evaluation = self.config.evaluation\n",
    "        params = self.params.training\n",
    "        create_directories([\n",
    "            Path(evaluation.root_dir)\n",
    "        ])\n",
    "        eval_config = EvaluationConfig(\n",
    "\n",
    "            mlflow_uri=\"https://dagshub.com/JAISON14/Audio-Based-Anomaly-Detection-for-Industrial-Machinery-End-to-End-Project-using-MLflow-DVC.mlflow\",\n",
    "            root_dir=Path(evaluation.root_dir),\n",
    "            feature_names_path=Path(evaluation.feature_names_path),\n",
    "            trained_model_path=Path(evaluation.trained_model_path),\n",
    "            feature_importance_path=Path(evaluation.feature_importance_path),\n",
    "            X_combined_test_path=Path(evaluation.X_combined_test_path),\n",
    "            y_combined_test_path=Path(evaluation.y_combined_test_path),\n",
    "            scores_path=Path(evaluation.scores_path),\n",
    "            all_params=self.params,\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_feature_count=params.FEATURE_COUNT\n",
    "        )\n",
    "        return eval_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import urllib.request as request\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from Anomaly_Detection import logger\n",
    "from Anomaly_Detection.utils.common import get_size,save_json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def model_evaluation(self,autoencoder,X_combined_test, y_combined_test,top_features):\n",
    "        reconstructed_combined = autoencoder.predict(X_combined_test)\n",
    "        mse_combined = np.mean(np.power(X_combined_test - reconstructed_combined, 2), axis=1)\n",
    "        # Calculate precision-recall curve\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_combined_test, mse_combined)\n",
    "\n",
    "        # Calculate F1 score for each threshold\n",
    "        f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # Use the optimal threshold to define anomalies\n",
    "        optimal_predictions = (mse_combined > optimal_threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics using the optimal threshold\n",
    "        optimal_accuracy = accuracy_score(y_combined_test, optimal_predictions)\n",
    "        optimal_precision = precision_score(y_combined_test, optimal_predictions)\n",
    "        optimal_recall = recall_score(y_combined_test, optimal_predictions)\n",
    "        optimal_f1 = f1_score(y_combined_test, optimal_predictions)\n",
    "        optimal_cm = confusion_matrix(y_combined_test, optimal_predictions)\n",
    "\n",
    "        self.save_score(optimal_threshold,optimal_accuracy,optimal_precision,optimal_recall,optimal_f1)\n",
    "\n",
    "        self.feature_importance(reconstructed_combined,X_combined_test,top_features)\n",
    "        \n",
    "        # Print metrics using the optimal threshold\n",
    "        logger.info(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "        logger.info(f\"Accuracy: {optimal_accuracy}\")\n",
    "        logger.info(f\"Precision: {optimal_precision}\")\n",
    "        logger.info(f\"Recall: {optimal_recall}\")\n",
    "        logger.info(f\"F1 Score: {optimal_f1}\")\n",
    "        logger.info(f\"confusion_matrix: {optimal_cm}\")   \n",
    "\n",
    "    def feature_importance(self,reconstructed_combined,X_combined_test,feature_names):\n",
    "        # Calculate the mean squared reconstruction error for each feature\n",
    "        mse_features = np.mean(np.power(X_combined_test - reconstructed_combined, 2), axis=0)\n",
    "        # Rank features by reconstruction error\n",
    "        feature_importance_ranking = np.argsort(mse_features)[::-1]  # Features with the highest error first\n",
    "        # Create a dictionary to store feature names and their importance scores\n",
    "        feature_importance = {}\n",
    "        for idx, feature_idx in enumerate(feature_importance_ranking):\n",
    "            feature_name = feature_names[feature_idx] \n",
    "            importance_score = mse_features[feature_idx]\n",
    "            feature_importance[feature_name] = importance_score\n",
    "        joblib.dump(feature_importance,(os.path.join(self.config.root_dir,\"feature_importance.pkl\")))\n",
    "        \n",
    "\n",
    "        \n",
    "    def feature_selection(self, N, feature_importance_ranking, feature_names):\n",
    "        top_features_indices = feature_importance_ranking[:N]\n",
    "        top_features=[]\n",
    "        for rank in feature_importance_ranking[:N]:\n",
    "            top_features.append(feature_names[rank])\n",
    "        return top_features,top_features_indices\n",
    "\n",
    "    def evaluation(self):\n",
    "        logger.info(f\"Starting Model Evaluation\")\n",
    "        feature_names = joblib.load(self.config.feature_names_path)\n",
    "\n",
    "        n = self.config.params_feature_count\n",
    "        feature_importance_ranking= joblib.load(self.config.feature_importance_path)\n",
    "\n",
    "        top_features,top_features_indices = self.feature_selection(n, feature_importance_ranking, feature_names)\n",
    "        X_combined_test = joblib.load(self.config.X_combined_test_path)\n",
    "        y_combined_test = joblib.load(self.config.y_combined_test_path)\n",
    "\n",
    "        autoencoder = load_model(self.config.trained_model_path)\n",
    "        self.model_evaluation(autoencoder,X_combined_test, y_combined_test,top_features)        \n",
    "   \n",
    "\n",
    "    def save_score(self,optimal_threshold,optimal_accuracy,optimal_precision,optimal_recall,optimal_f1):\n",
    "        scores = {\"Optimal Threshold\": optimal_threshold, \"Accuracy\": optimal_accuracy,\n",
    "                  \"Precision\": optimal_precision , \"Recall\": optimal_recall,\n",
    "                  \"F1 Score\": optimal_f1\n",
    "                  }\n",
    "        #joblib.dump(scores,(os.path.join(self.config.root_dir, \"scores.pkl\")))\n",
    "        joblib.dump(scores,self.config.scores_path)\n",
    "        save_json(path=Path((os.path.join(self.config.root_dir, \"scores.json\"))), data=scores)\n",
    "\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            feature_importance=joblib.load((os.path.join(self.config.root_dir,\"feature_importance.pkl\")))\n",
    "            mlflow.log_params(feature_importance)\n",
    "\n",
    "            #scores=joblib.load((os.path.join(self.config.root_dir, \"scores.pkl\")))\n",
    "            scores=joblib.load(self.config.scores_path)\n",
    "\n",
    "            # Load the model\n",
    "            model = load_model(self.config.trained_model_path)\n",
    "            \n",
    "            mlflow.log_metrics(\n",
    "                scores\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.keras.log_model(model, \"model\", registered_model_name=\"Top 5 Features\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 23:00:22,189: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-12-04 23:00:22,197: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-04 23:00:22,202: INFO: common: created directory at: artifacts]\n",
      "[2023-12-04 23:00:22,209: INFO: common: created directory at: artifacts\\evaluation]\n",
      "[2023-12-04 23:00:22,212: INFO: 1110878222: Starting Model Evaluation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 5ms/step\n",
      "[2023-12-04 23:00:25,009: INFO: common: json file saved at: artifacts\\evaluation\\scores.json]\n",
      "[2023-12-04 23:00:25,014: INFO: 1110878222: Optimal Threshold: 1.26537117259149]\n",
      "[2023-12-04 23:00:25,017: INFO: 1110878222: Accuracy: 0.9488372093023256]\n",
      "[2023-12-04 23:00:25,023: INFO: 1110878222: Precision: 0.9847328244274809]\n",
      "[2023-12-04 23:00:25,029: INFO: 1110878222: Recall: 0.9347826086956522]\n",
      "[2023-12-04 23:00:25,039: INFO: 1110878222: F1 Score: 0.9591078066914499]\n",
      "[2023-12-04 23:00:25,051: INFO: 1110878222: confusion_matrix: [[ 75   2]\n",
      " [  9 129]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/04 23:00:28 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 23:00:34,262: WARNING: save: Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.]\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Jaison\\AppData\\Local\\Temp\\tmphr2ikipv\\model\\data\\model\\assets\n",
      "[2023-12-04 23:00:37,052: INFO: builder_impl: Assets written to: C:\\Users\\Jaison\\AppData\\Local\\Temp\\tmphr2ikipv\\model\\data\\model\\assets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Top 5 Features' already exists. Creating a new version of this model...\n",
      "2023/12/04 23:01:16 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: Top 5 Features, version 3\n",
      "Created version '3' of model 'Top 5 Features'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.evaluation()\n",
    "    evaluation.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "   raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
